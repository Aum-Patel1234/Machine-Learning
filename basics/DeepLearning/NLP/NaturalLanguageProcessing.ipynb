{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ogAr9JEfmv6"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "## **What is NLP?**\n",
        "**NLP stands for Natural Language Processing**, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
        "\n",
        "![alt text](https://lh4.googleusercontent.com/HvOsRDR2W55YFM1ffjeBAmCr1hUVeQlT_h2NZXd_dQCdqiaKTDeWIUg28wmznHhI9WhO2tc6q6DVu5SMmfKtfhiCiEocD82JFWSnpich9okxrfBtbwJynbbe8FgUF2vdT2S0Fb0)\n",
        "\n",
        "## **Applications of NLP**\n",
        "There are the following applications of NLP -\n",
        "1. **Question Answering**\n",
        "\n",
        "Question Answering focuses on building systems that automatically answer the questions asked by humans in a natural language.\n",
        "\n",
        "![alt text](https://lh4.googleusercontent.com/llxua0Kxt4LY6Jq0KtwdoLqVp9VmGYtpU7SXy71KHB4tW7CRGT8tQ2xGmaoTZ3gX0CC5TQgRyuKEQttbUVB30yNQuI7R1HcEJKSKNms8BhqBEVnbst7w2KI6UIGzH81Fev77SWQ)\n",
        "\n",
        "2. **Spam Detection**\n",
        "\n",
        "Spam detection is used to detect unwanted emails getting to a user's inbox.\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/OMoXE4C6lwKdM0QE55FjxKu8c6pqWt7-5P1AcGfqQPJuSKGjG0xHqKJNtvH9Fo16DbeQLki8uXGuBiblLYXHQE54Xru-hFysYLc1By3vnrRfD33qA51xx1zNnxANhZlaE5aeh-M)\n",
        "\n",
        "3. **Sentiment Analysis**\n",
        "\n",
        "Sentiment Analysis is also known as opinion mining. It is used on the web to analyse the attitude, behaviour, and emotional state of the sender. This application is implemented through a combination of NLP (Natural Language Processing) and statistics by assigning the values to the text (positive, negative, or natural), identify the mood of the context (happy, sad, angry, etc.)\n",
        "\n",
        "![alt text](https://lh6.googleusercontent.com/ochS5RaVmHfIQyEML5QmtH1G9dXq9Oao3a9K7QuaQ_4Zn7EN5LzVg2Rv1cOJgpoNSdpLWqIZf_2VBeop3cmie3WeG0La9aCc4NkZ5fS_01aAwsJ6ACTi31G_xXe-73rdNISPqiM)\n",
        "\n",
        "4. **Machine Translation**\n",
        "\n",
        "Machine translation is used to translate text or speech from one natural language to another natural language.\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/AgvZMNyu7VEKSi2O-1gWlx82EqLCZdT5EmVxF4rEPokG2JJy8fSyPgMEVK5juLDTnkz5WVBQ_Nm6TEmXVrnOg5T2C7dYr2JDbanzPaKOWXINCy0uolVIx-Irez-_0rVl3ao3Eds)\n",
        "\n",
        "5. **Spelling correction**\n",
        "\n",
        "Microsoft Corporation provides word processor software like MS-word, PowerPoint for the spelling correction.\n",
        "\n",
        "![alt text](https://lh5.googleusercontent.com/nvn5xkiQ4lncZKob0z5fyMmBIyPyZZu71ltKvXh8qrc9lauc_et6Lt89drEjngOphnUv2xCLUQRkgry0qYF_Sdw4sw7u0g0sjMCRK9X8akuhGHc3VLYS39J4MwxHspHCiGuHkf4)\n",
        "\n",
        "6. **Speech Recognition**\n",
        "\n",
        "Speech recognition is used for converting spoken words into text. It is used in applications, such as mobile, home automation, video recovery, dictating to Microsoft Word, voice biometrics, voice user interface, and so on.\n",
        "\n",
        "7. **Chatbot**\n",
        "\n",
        "Implementing the Chatbot is one of the important applications of NLP. It is used by many companies to provide the customer's chat services.\n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/o5A5kbmvoKElNZCsLSlv0aFyWw9Un4tINXuh98RQ-pCkjQvZRujkj7y_XaAZ73BL9wfMZH36ElKb7XOieagpHEJPQSWBLSur2hLAzw7tvypqw7vTqe8kGLv0LabwncV3F3R8glE)\n",
        "\n",
        "8. **Information extraction**\n",
        "\n",
        "Information extraction is one of the most important applications of NLP. It is used for extracting structured information from unstructured or semi-structured machine-readable documents.\n",
        "\n",
        "9. **Natural Language Understanding (NLU)**\n",
        "\n",
        "It converts a large set of text into more formal representations such as first-order logic structures that are easier for the computer programs to manipulate notations of the natural language processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dttHnQkfm1v"
      },
      "source": [
        "There are various techniques of doing NLP, here we will study the Bag of words model.\n",
        "\n",
        "Whenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm. Hence, the Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words.\n",
        "\n",
        "## **Problems we face while NLP:**\n",
        "**Categorical data**\n",
        "\n",
        "Since in NLP we deal exclusively with text data, it is very important to handle it as we are already aware that we cannot pass categorical data to the ML model. A ML model works on algorithms which consist of various mathematical and statistical formulas and we cannot pass text as input for a mathematical formula.\n",
        "\n",
        "**No fixed length**\n",
        "\n",
        "In NLP we cannot estimate the length of the data. We have to use free flowing data.\n",
        "\n",
        "## **Flow of Analysing Text data:**\n",
        "\n",
        "![alt text](https://docs.google.com/drawings/u/0/d/s62D0wOlx8O7edOmTbyHsUw/image?w=341&h=356&rev=1&ac=1&parent=1OSwv1l6BFj2A0EBicPrt6iK4Y3p0Y-Lc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_yV7jjmiL9-"
      },
      "source": [
        "Problem Statement example for Natural Language Processing:\n",
        "Here we have a text file that contains the reviews for a particular restaurant. We have mentioned whether the review is a positive (1) or negative (0) after the review with a **tab (/t )** separator.\n",
        "\n",
        "```\n",
        "Review    Liked\n",
        "Wow... Loved this place.\t1\n",
        "Crust is not good.\t0\n",
        "Not tasty and the texture was just nasty.\t0\n",
        "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\t1\n",
        "The selection on the menu was great and so were the prices.\t1\n",
        "Now I am getting angry and I want my damn pho.\t0\n",
        "Honestly it didn't taste THAT fresh.)\t0\n",
        "The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\t0\n",
        "The fries were great too.\t1\n",
        "A great touch.\t1\n",
        "Service was very prompt.\t1\n",
        "Would not go back.\t0\n",
        "The cashier had no care what so ever on what I had to say it still ended up being wayyy overpriced.\t0\n",
        "I tried the Cape Cod ravoli, chicken, with cranberry...mmmm!\t1\n",
        "I was disgusted because I was pretty sure that was human hair.\t0\n",
        "I was shocked because no signs indicate cash only.\t0\n",
        ".\n",
        ".\n",
        "```\n",
        "### **Step 1: Libraries, reading the data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uhJ9LCWxfnAB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "h35OQJ4efnFX"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('./data/Restaurant_Reviews.tsv',sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zaUEYvMNjqhh",
        "outputId": "75132f74-4894-483e-b566-56e9774bf8cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>I think food should have flavor and texture an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Appetite instantly gone.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Overall I was not impressed and would not go b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>The whole experience was underwhelming, and I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Review  Liked\n",
              "0                             Wow... Loved this place.      1\n",
              "1                                   Crust is not good.      0\n",
              "2            Not tasty and the texture was just nasty.      0\n",
              "3    Stopped by during the late May bank holiday of...      1\n",
              "4    The selection on the menu was great and so wer...      1\n",
              "..                                                 ...    ...\n",
              "995  I think food should have flavor and texture an...      0\n",
              "996                           Appetite instantly gone.      0\n",
              "997  Overall I was not impressed and would not go b...      0\n",
              "998  The whole experience was underwhelming, and I ...      0\n",
              "999  Then, as if I hadn't wasted enough of my life ...      0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-UGNsHj0CY",
        "outputId": "2b8d6f02-73ed-414e-848b-94bb53cbc629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Wow... Loved this place.', 'Crust is not good.',\n",
              "       'Not tasty and the texture was just nasty.',\n",
              "       'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.',\n",
              "       'The selection on the menu was great and so were the prices.',\n",
              "       'Now I am getting angry and I want my damn pho.',\n",
              "       \"Honeslty it didn't taste THAT fresh.)\",\n",
              "       'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.',\n",
              "       'The fries were great too.', 'A great touch.'], dtype=object)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review = dataset['Review'].values\n",
        "review[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wow    Loved this place \n",
            "Wow Loved this place "
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "rev = re.sub('[^a-zA-Z]', ' ', 'Wow... Loved this place.')\n",
        "print(rev)\n",
        "for word in rev.split(' '):\n",
        "    if word != '':\n",
        "        print(word, end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK4WjGjjivwA"
      },
      "source": [
        "In the bag of words technique we depend on keywords to identify whether it is a positive comment or negative.\n",
        "\n",
        "All positive keywords are in a bag of words.\n",
        "\n",
        "### **Step 2:  Get rid of unnecessary words/ symbols.**\n",
        "For this we will use the library of **re (regular expressions)**\n",
        "\n",
        "The **re library** has a function **substitute**, here we will mention the following parameters as follows :\n",
        "\n",
        "What to substitute\n",
        "Replace with what\n",
        "Where\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGeLI_tMfnIf",
        "outputId": "439f18e9-5f83-423e-e9d2-c53aa456360d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Wow    Loved this place ',\n",
              " 'Crust is not good ',\n",
              " 'Not tasty and the texture was just nasty ',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it ',\n",
              " 'The selection on the menu was great and so were the prices ',\n",
              " 'Now I am getting angry and I want my damn pho ',\n",
              " 'Honeslty it didn t taste THAT fresh  ',\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer ',\n",
              " 'The fries were great too ',\n",
              " 'A great touch ']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re     # regular expression\n",
        "\n",
        "filtered_text = []\n",
        "\n",
        "for rev in review:\n",
        "  rev = re.sub('[^a-zA-Z]', ' ', rev)\n",
        "  filtered_text.append(rev)\n",
        "\n",
        "review = filtered_text\n",
        "review[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k36kJDPYjI8A"
      },
      "source": [
        "### **Step 3: Convert text into lowercase and a list of words.**\n",
        "To convert in lowercase we will use the function **lower()** and to convert into a list of words we will use **split()**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymUDF79zfnL5",
        "outputId": "c0b0c822-b62d-41cb-a066-759bad5a980b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['wow', 'loved', 'this', 'place'],\n",
              " ['crust', 'is', 'not', 'good'],\n",
              " ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty'],\n",
              " ['stopped',\n",
              "  'by',\n",
              "  'during',\n",
              "  'the',\n",
              "  'late',\n",
              "  'may',\n",
              "  'bank',\n",
              "  'holiday',\n",
              "  'off',\n",
              "  'rick',\n",
              "  'steve',\n",
              "  'recommendation',\n",
              "  'and',\n",
              "  'loved',\n",
              "  'it'],\n",
              " ['the',\n",
              "  'selection',\n",
              "  'on',\n",
              "  'the',\n",
              "  'menu',\n",
              "  'was',\n",
              "  'great',\n",
              "  'and',\n",
              "  'so',\n",
              "  'were',\n",
              "  'the',\n",
              "  'prices'],\n",
              " ['now',\n",
              "  'i',\n",
              "  'am',\n",
              "  'getting',\n",
              "  'angry',\n",
              "  'and',\n",
              "  'i',\n",
              "  'want',\n",
              "  'my',\n",
              "  'damn',\n",
              "  'pho'],\n",
              " ['honeslty', 'it', 'didn', 't', 'taste', 'that', 'fresh'],\n",
              " ['the',\n",
              "  'potatoes',\n",
              "  'were',\n",
              "  'like',\n",
              "  'rubber',\n",
              "  'and',\n",
              "  'you',\n",
              "  'could',\n",
              "  'tell',\n",
              "  'they',\n",
              "  'had',\n",
              "  'been',\n",
              "  'made',\n",
              "  'up',\n",
              "  'ahead',\n",
              "  'of',\n",
              "  'time',\n",
              "  'being',\n",
              "  'kept',\n",
              "  'under',\n",
              "  'a',\n",
              "  'warmer'],\n",
              " ['the', 'fries', 'were', 'great', 'too'],\n",
              " ['a', 'great', 'touch']]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "altered_text = []\n",
        "\n",
        "for rev in review:\n",
        "  rev = rev.lower()\n",
        "  rev = rev.split()\n",
        "  altered_text.append(rev)\n",
        "\n",
        "review = altered_text\n",
        "review[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bGtXaRUj_IQ"
      },
      "source": [
        "### **Step 4: Using stopwords.**\n",
        "Stopwords: These are the unnecessary words that donot have a specific meaning.\n",
        "**Eg. this, is, and, the, a, an etc**\n",
        "\n",
        "We will download these stopwords from the library **nltk (Natural Language Toolkit Library)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/aum/anaconda3/envs/ml/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /home/aum/anaconda3/envs/ml/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/aum/anaconda3/envs/ml/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/aum/anaconda3/envs/ml/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /home/aum/anaconda3/envs/ml/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBwSsG-6j9_P",
        "outputId": "d8d9ca76-f4e4-4ab1-bc03-8bdd53be4726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/aum/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6_5xT07GkQI5"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his himself she she's her hers herself it it's its itself they them their theirs themselves what which who whom this that that'll these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don don't should should've now d ll m o re ve y ain aren aren't couldn couldn't didn didn't doesn doesn't hadn hadn't hasn hasn't haven haven't isn isn't ma mightn mightn't mustn mustn't needn needn't shan shan't shouldn shouldn't wasn wasn't weren weren't won won't wouldn wouldn't "
          ]
        }
      ],
      "source": [
        "for word in stopwords.words('english'):\n",
        "    print(word, end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rx347uVkuN7"
      },
      "source": [
        "### **Step 5: Getting rid of stopwords.**\n",
        "For this we will create an empty **list mybag[]** and store all the useful words i.e words other than the stopwords in that list.\n",
        "\n",
        "**Note:** Some words which are present in stopwords but may be useful. Hence, we will create another list **notstopwords** and store those words in that list.\n",
        "\n",
        "**Eg. not** is present in the list of stopwords. But without ‘not’ the meaning of a review can completely change\n",
        "\n",
        "**The gravy was not good. → (without not) The gravy was good.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Ma1tfijjkQMS"
      },
      "outputs": [],
      "source": [
        "notStopWords = ['not']\n",
        "my_bag = []\n",
        "\n",
        "for data in review:\n",
        "  my_short_bag = []\n",
        "  for data_text in data:\n",
        "    if data_text not in stopwords.words('english'):\n",
        "      my_short_bag.append(data_text)\n",
        "\n",
        "    if data_text in notStopWords:\n",
        "      my_short_bag.append(data_text)\n",
        "\n",
        "  my_bag.append(my_short_bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp1sNGVBkQRD",
        "outputId": "31a02bec-f9e8-44d9-e258-1b59a0deecbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['wow', 'loved', 'place'],\n",
              " ['crust', 'not', 'good'],\n",
              " ['not', 'tasty', 'texture', 'nasty'],\n",
              " ['stopped',\n",
              "  'late',\n",
              "  'may',\n",
              "  'bank',\n",
              "  'holiday',\n",
              "  'rick',\n",
              "  'steve',\n",
              "  'recommendation',\n",
              "  'loved'],\n",
              " ['selection', 'menu', 'great', 'prices'],\n",
              " ['getting', 'angry', 'want', 'damn', 'pho'],\n",
              " ['honeslty', 'taste', 'fresh'],\n",
              " ['potatoes',\n",
              "  'like',\n",
              "  'rubber',\n",
              "  'could',\n",
              "  'tell',\n",
              "  'made',\n",
              "  'ahead',\n",
              "  'time',\n",
              "  'kept',\n",
              "  'warmer'],\n",
              " ['fries', 'great'],\n",
              " ['great', 'touch']]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_bag[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJVmMLn8oGoD"
      },
      "source": [
        "### **Step 6: Steaming**\n",
        "Steaming is the process of replacing the words with the original word from which it has originated.\n",
        "\n",
        "**Example: loved, loving → love**\n",
        "\n",
        "For this we use the library **PorterStemmer from nltk**.\n",
        "\n",
        "We will create an empty list to store all the steam words. So that we get a unique list of stem words. We will first create an object of the **PorterStemmer** class and then use a for loop to convert the words from mybag to its stem word and then add it to the **mystembag[]**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AmXERvFSoGyw"
      },
      "outputs": [],
      "source": [
        "mystembag = []\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKmfX7Zxomhq",
        "outputId": "269175b4-4990-4764-c585-0e3bd873e267"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['wow', 'love', 'place'],\n",
              " ['crust', 'not', 'good'],\n",
              " ['not', 'tasti', 'textur', 'nasti'],\n",
              " ['stop',\n",
              "  'late',\n",
              "  'may',\n",
              "  'bank',\n",
              "  'holiday',\n",
              "  'rick',\n",
              "  'steve',\n",
              "  'recommend',\n",
              "  'love'],\n",
              " ['select', 'menu', 'great', 'price'],\n",
              " ['get', 'angri', 'want', 'damn', 'pho'],\n",
              " ['honeslti', 'tast', 'fresh'],\n",
              " ['potato',\n",
              "  'like',\n",
              "  'rubber',\n",
              "  'could',\n",
              "  'tell',\n",
              "  'made',\n",
              "  'ahead',\n",
              "  'time',\n",
              "  'kept',\n",
              "  'warmer'],\n",
              " ['fri', 'great'],\n",
              " ['great', 'touch']]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for data in my_bag:\n",
        "  my_short_stembag = []\n",
        "  ps = PorterStemmer()\n",
        "  for data_text in data:\n",
        "    stem_word = ps.stem(data_text)\n",
        "    if stem_word not in my_short_stembag:\n",
        "      my_short_stembag.append(stem_word)\n",
        "\n",
        "  mystembag.append(my_short_stembag)\n",
        "\n",
        "mystembag[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV5tS6MPpjcq",
        "outputId": "5653cdc3-7403-4cd2-cda9-2685cd7ef625"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wow love place',\n",
              " 'crust not good',\n",
              " 'not tasti textur nasti',\n",
              " 'stop late may bank holiday rick steve recommend love',\n",
              " 'select menu great price',\n",
              " 'get angri want damn pho',\n",
              " 'honeslti tast fresh',\n",
              " 'potato like rubber could tell made ahead time kept warmer',\n",
              " 'fri great',\n",
              " 'great touch']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_review = []\n",
        "\n",
        "for rev in mystembag:\n",
        "  rev = ' '.join(rev)\n",
        "  final_review.append(rev)\n",
        "\n",
        "review = final_review\n",
        "review[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "NJYtC_O0pqkL"
      },
      "outputs": [],
      "source": [
        "# corups is collection of all stem reviews...\n",
        "corpus = []\n",
        "\n",
        "for rev in review:\n",
        "  corpus.append(rev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb8DUlAlzceH",
        "outputId": "845f8ad6-b478-46b0-f269-5f63cd95a43a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wow love place',\n",
              " 'crust not good',\n",
              " 'not tasti textur nasti',\n",
              " 'stop late may bank holiday rick steve recommend love',\n",
              " 'select menu great price',\n",
              " 'get angri want damn pho',\n",
              " 'honeslti tast fresh',\n",
              " 'potato like rubber could tell made ahead time kept warmer',\n",
              " 'fri great',\n",
              " 'great touch']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-h3Airaq0P9"
      },
      "source": [
        "## **sklearn.feature_extraction.text.CountVectorizer**\n",
        "\n",
        "Convert a collection of text documents to a matrix of token counts\n",
        "\n",
        "This implementation produces a sparse representation of the counts using **scipy.sparse.csr_matrix**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. wow great\n",
        "# 2. nice man\n",
        "\n",
        "# corpus = [wow, great, nice, man]\n",
        "\n",
        "# countrvectors  = [wow, great, nice, man] => [0, 1, 2, 3]\n",
        "\n",
        "# nice great wow great = [1, 2, 1, 0]\n",
        "\n",
        "# 1. wow great => [1, 1, 0, 0]\n",
        "# 2. nice man => [0, 0, 1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Xt4clX0epqn6"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tUtqHj_6pqrK"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1566"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(cv.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(X[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "print(sum(list(X[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 802\n",
            "1 1029\n",
            "1 1548\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1566"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for idx, x in enumerate(list(X[0])):\n",
        "    if x == 1:\n",
        "        print(x, idx)\n",
        "\n",
        "len(X[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "sdWOO9Rgpquz"
      },
      "outputs": [],
      "source": [
        "Y = dataset.iloc[:, 1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ngz2Byzepqzm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 47, stratify = Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey4fNBIE0Rhl",
        "outputId": "1bd61fca-5173-4a68-e88c-e78727e6dafe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_clf = RandomForestClassifier(n_estimators = 1)\n",
        "random_clf.fit(X_train, Y_train)\n",
        "Y_pred = random_clf.predict(X_test)\n",
        "Y_pred[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w04S-Ic1PwS",
        "outputId": "5b9a83ae-2d89-4d23-91ce-e397067b4177"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9057142857142857"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_clf.score(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocj9ASSr0tQN",
        "outputId": "eae80416-2e28-4242-8277-3856290ace8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training accuracy:  0.9057142857142857\n",
            "testing accuracy:  0.6833333333333333\n"
          ]
        }
      ],
      "source": [
        "print('training accuracy: ', random_clf.score(X_train, Y_train))\n",
        "print('testing accuracy: ', random_clf.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "LZPAS2sGk0-S"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrpx6vFLlKX8",
        "outputId": "ddaf23d4-eee5-4654-ac0c-02f0dd9c5f93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "support_classifier = SVC()\n",
        "support_classifier.fit(X_train, Y_train)\n",
        "y_prediction = support_classifier.predict(X_test)\n",
        "y_prediction[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIE0xWh8k1CF",
        "outputId": "827e64e1-dbca-4b89-d9eb-d854cd356090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training accuracy:  0.98\n",
            "testing accuracy:  0.7733333333333333\n"
          ]
        }
      ],
      "source": [
        "print('training accuracy: ', support_classifier.score(X_train, Y_train))\n",
        "print('testing accuracy: ', support_classifier.score(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
